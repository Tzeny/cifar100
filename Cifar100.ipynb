{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Cifar100.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [
        "nZ-oiI4YlXGL",
        "sqKMRov-lQjI",
        "l-LAEOiUmMgt"
      ],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "[View in Colaboratory](https://colab.research.google.com/github/Tzeny/cifar100/blob/master/Cifar100.ipynb)"
      ]
    },
    {
      "metadata": {
        "id": "509BEESjqAt5",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "This is my attempt at solving the Cifar100 challenge. This file is also available on GitHub: [https://github.com/Tzeny/cifar10/blob/master/Cifar10.ipynb](https://github.com/Tzeny/cifar10/blob/master/Cifar10.ipynb)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "ZU99-wAUrBei",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "|Nr. crt.|Model architecture | Optimizer | Batch size  | Train images | Stop epoch | Test accuracy |\n",
        "|---||------------- |---|-------------|----|\n",
        "|1|2 x inception(w dr 3,36) 2 x (conv(144ft)+maxpool) flt 2 x dense (w dr) act \\[w batch normalization\\]|Adam(0.001)|128|Normalized color |~20|~93 %"
      ]
    },
    {
      "metadata": {
        "id": "mIYeok5Dp-YC",
        "colab_type": "toc"
      },
      "cell_type": "markdown",
      "source": [
        ">[Google Colab ensure we have our own GPU](#scrollTo=nZ-oiI4YlXGL)\n",
        "\n",
        ">[Connecting to Google Drive](#scrollTo=sqKMRov-lQjI)\n",
        "\n",
        ">[Prepare our dataset](#scrollTo=l-LAEOiUmMgt)\n",
        "\n",
        ">[Model definition and training](#scrollTo=xmoQDAurmSbS)\n",
        "\n",
        ">[Model evaluation](#scrollTo=f53m3bg_p5rk)\n",
        "\n"
      ]
    },
    {
      "metadata": {
        "id": "nZ-oiI4YlXGL",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Google Colab ensure we have our own GPU"
      ]
    },
    {
      "metadata": {
        "id": "abWGmIbfhggR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 0
        },
        "outputId": "224ce73f-d781-4a38-8a41-218a1e29997d"
      },
      "cell_type": "code",
      "source": [
        "!ln -sf /opt/bin/nvidia-smi /usr/bin/nvidia-smi\n",
        "!pip install gputil\n",
        "!pip install psutil\n",
        "!pip install humanize\n",
        "import psutil\n",
        "import humanize\n",
        "import os\n",
        "import GPUtil as GPU\n",
        "GPUs = GPU.getGPUs()\n",
        "# XXX: only one GPU on Colab and isn’t guaranteed\n",
        "gpu = GPUs[0]\n",
        "def printm():\n",
        "  process = psutil.Process(os.getpid())\n",
        "  print(\"Gen RAM Free: \" + humanize.naturalsize( psutil.virtual_memory().available ), \" | Proc size: \" + humanize.naturalsize( process.memory_info().rss))\n",
        "  print(\"GPU RAM Free: {0:.0f}MB | Used: {1:.0f}MB | Util {2:3.0f}% | Total {3:.0f}MB\".format(gpu.memoryFree, gpu.memoryUsed, gpu.memoryUtil*100, gpu.memoryTotal))\n",
        "printm()"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: gputil in /usr/local/lib/python3.6/dist-packages (1.3.0)\r\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from gputil) (1.14.5)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.6/dist-packages (5.4.6)\n",
            "Requirement already satisfied: humanize in /usr/local/lib/python3.6/dist-packages (0.5.1)\n",
            "Gen RAM Free: 12.6 GB  | Proc size: 139.9 MB\n",
            "GPU RAM Free: 11438MB | Used: 1MB | Util   0% | Total 11439MB\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6U2APHXm65Q4",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        " !kill -9 -1"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "sqKMRov-lQjI",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Connecting to Google Drive"
      ]
    },
    {
      "metadata": {
        "id": "QiochJEglOwI",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 124
        },
        "outputId": "9faba428-d20b-49c8-f61d-65022e113692"
      },
      "cell_type": "code",
      "source": [
        "# Install a Drive FUSE wrapper.\n",
        "# https://github.com/astrada/google-drive-ocamlfuse\n",
        "!apt-get install -y -qq software-properties-common python-software-properties module-init-tools\n",
        "!add-apt-repository -y ppa:alessandro-strada/ppa 2>&1 > /dev/null\n",
        "!apt-get update -qq 2>&1 > /dev/null\n",
        "!apt-get -y install -qq google-drive-ocamlfuse fuse\n",
        "\n",
        "# Generate auth tokens for Colab\n",
        "from google.colab import auth\n",
        "auth.authenticate_user()\n",
        "\n",
        "# Generate creds for the Drive FUSE library.\n",
        "from oauth2client.client import GoogleCredentials\n",
        "creds = GoogleCredentials.get_application_default()\n",
        "import getpass\n",
        "!google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret} < /dev/null 2>&1 | grep URL\n",
        "vcode = getpass.getpass()\n",
        "!echo {vcode} | google-drive-ocamlfuse -headless -id={creds.client_id} -secret={creds.client_secret}\n",
        "\n",
        "# Create a directory and mount Google Drive using that directory.\n",
        "!mkdir -p my_drive\n",
        "!google-drive-ocamlfuse my_drive\n",
        "!ls my_drive/ai\n"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\r\n",
            "··········\n",
            "Please, open the following URL in a web browser: https://accounts.google.com/o/oauth2/auth?client_id=32555940559.apps.googleusercontent.com&redirect_uri=urn%3Aietf%3Awg%3Aoauth%3A2.0%3Aoob&scope=https%3A%2F%2Fwww.googleapis.com%2Fauth%2Fdrive&response_type=code&access_type=offline&approval_prompt=force\n",
            "Please enter the verification code: Access token retrieved correctly.\n",
            "kaggle\tprojects\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "l-LAEOiUmMgt",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Prepare our dataset"
      ]
    },
    {
      "metadata": {
        "id": "M3yjSJHMh-RR",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "423a3212-e099-4d73-b498-13b0f17b4478"
      },
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "%matplotlib inline\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "np.random.seed(451)\n",
        "\n",
        "from keras.datasets import cifar100\n",
        "\n",
        "(X_train, y_train), (X_test, y_test) = cifar100.load_data()\n",
        "\n",
        "# print shape of data while model is building\n",
        "print(\"{1} train samples, {2} channel{0}, {3}x{4}\".format(\"\" if X_train.shape[1] == 1 else \"s\", *X_train.shape))\n",
        "print(\"{1}  test samples, {2} channel{0}, {3}x{4}\".format(\"\" if X_test.shape[1] == 1 else \"s\", *X_test.shape))\n",
        "\n",
        "#capital letter values will be used for the generators\n",
        "X_train, X_valid, Y_train, Y_valid = train_test_split(x_train, y_train, test_size=0.2)\n",
        "\n",
        "Y_train_cat = to_categorical(Y_train)\n",
        "Y_test_cat = to_categorical(y_test)\n",
        "Y_valid_cat = to_categorical(Y_valid)"
      ],
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "50000 train samples, 32 channels, 32x3\n",
            "10000  test samples, 32 channels, 32x3\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "95CvWHBHJ964",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 295
        },
        "outputId": "ffae3876-3904-4153-c441-35999c346b0c"
      },
      "cell_type": "code",
      "source": [
        "def unpickle(file):\n",
        "    import pickle\n",
        "    with open(file, 'rb') as fo:\n",
        "        dict = pickle.load(fo, encoding='bytes')\n",
        "    return dict\n",
        "  \n",
        "!wget https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
        "!tar -xvf cifar-100-python.tar.gz"
      ],
      "execution_count": 54,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2018-07-12 11:03:36--  https://www.cs.toronto.edu/~kriz/cifar-100-python.tar.gz\n",
            "Resolving www.cs.toronto.edu (www.cs.toronto.edu)... 128.100.3.30\n",
            "Connecting to www.cs.toronto.edu (www.cs.toronto.edu)|128.100.3.30|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 169001437 (161M) [application/x-gzip]\n",
            "Saving to: ‘cifar-100-python.tar.gz’\n",
            "\n",
            "cifar-100-python.ta 100%[===================>] 161.17M  5.04MB/s    in 43s     \n",
            "\n",
            "2018-07-12 11:04:19 (3.74 MB/s) - ‘cifar-100-python.tar.gz’ saved [169001437/169001437]\n",
            "\n",
            "cifar-100-python/\n",
            "cifar-100-python/file.txt~\n",
            "cifar-100-python/train\n",
            "cifar-100-python/test\n",
            "cifar-100-python/meta\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "zZbfspnbK-wL",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "train = unpickle('cifar-100-python/train')\n",
        "test = unpickle('cifar-100-python/test')\n",
        "\n",
        "meta = unpickle('cifar-100-python/meta')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "igyRw5y4LNm3",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "classes = meta[b'fine_label_names']"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "SHfOGuh9FSuu",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "1ee64dbb-7854-466f-d0d8-e9e70a613f03"
      },
      "cell_type": "code",
      "source": [
        "print(X_train.shape,X_valid.shape,x_test.shape)\n",
        "print(Y_train_cat.shape,Y_valid_cat.shape,Y_test_cat.shape)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(40000, 32, 32, 3) (10000, 32, 32, 3) (10000, 32, 32, 3)\n",
            "(40000, 100) (10000, 100) (10000, 100)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "6vRrAyUVEFkJ",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 301
        },
        "outputId": "1d17a5d6-9a2a-4e00-e8ef-0118cefb930a"
      },
      "cell_type": "code",
      "source": [
        "import random\n",
        "i = random.randint(0,10000)\n",
        "\n",
        "print(classes[np.argmax(Y_valid_cat[i])])\n",
        "plt.imshow(X_valid[i])"
      ],
      "execution_count": 91,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "b'clock'\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<matplotlib.image.AxesImage at 0x7f021ff56a20>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 91
        },
        {
          "output_type": "display_data",
          "data": {
            "image/png": "iVBORw0KGgoAAAANSUhEUgAAAPoAAAD5CAYAAAAOeCiTAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nYII=\n",
            "text/plain": [
              "<matplotlib.figure.Figure at 0x7f022019c2e8>"
            ]
          },
          "metadata": {
            "tags": []
          }
        }
      ]
    },
    {
      "metadata": {
        "id": "R0pBbZ1HVK5I",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from keras.preprocessing.image import ImageDataGenerator\n",
        "\n",
        "generator = ImageDataGenerator(rescale = 1./255 )\n",
        "\n",
        "valid = generator.flow(X_valid,Y_valid_cat,batch_size = 64)\n",
        "\n",
        "test = generator.flow(x_test,Y_test_cat,batch_size = 64)\n",
        "\n",
        "generator2 = ImageDataGenerator(rescale = 1./255)\n",
        "\n",
        "train = generator2.flow(X_train,Y_train_cat,batch_size = 64)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "xmoQDAurmSbS",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model definition and training"
      ]
    },
    {
      "metadata": {
        "id": "KqmcChpEiFJB",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2190
        },
        "outputId": "679814e2-8641-4111-c7a9-4f6c6898326f"
      },
      "cell_type": "code",
      "source": [
        "np.random.seed(451)\n",
        "\n",
        "import datetime\n",
        "\n",
        "from keras.layers import Flatten, Activation, Conv2D, MaxPool2D, AvgPool2D, Dense, Dropout, BatchNormalization, Input, MaxPooling2D, Flatten, Activation, Conv2D, AvgPool2D, Dense, Dropout, concatenate, AveragePooling2D\n",
        "from keras.optimizers import Adam, SGD\n",
        "from keras.models import Sequential\n",
        "import keras.backend as K\n",
        "from keras.regularizers import l1,l2\n",
        "from keras.callbacks import EarlyStopping, ModelCheckpoint, TensorBoard, ReduceLROnPlateau\n",
        "from keras.models import model_from_json, Model\n",
        "\n",
        "def build_tower(input_layer, features_nr, shape, tower_nr, \n",
        "                dropout=False, normalization=False, regularization=\"l2\", dropout_ratio=0.25):\n",
        "    #3x3 kernel tower\n",
        "    tower = Conv2D(features_nr, (1,1), padding='same', activation='relu', \n",
        "                     kernel_regularizer=regularization, name='tower_%d_%dx%da'%(tower_nr, shape[0], shape[1]))(input_layer)\n",
        "    tower = Conv2D(features_nr*2, shape, padding='same', activation='relu',\n",
        "                     kernel_regularizer=regularization, name='tower_%d_%dx%db'%(tower_nr, shape[0], shape[1]))(tower)\n",
        "    #condidional dropout/normalization\n",
        "    if dropout:\n",
        "        tower = Dropout(dropout_ratio, name='tower_%d_%dx%ddrop'%(tower_nr, shape[0], shape[1]))(tower)\n",
        "    if normalization:\n",
        "        tower = BatchNormalization(name='tower_%d_%dx%dnorm'%(tower_nr, shape[0], shape[1]))(tower)\n",
        "        \n",
        "    return tower\n",
        "\n",
        "def build_simple_tower(input_layer, features_nr, shape, tower_nr, \n",
        "                dropout=False, normalization=False, regularization=\"l2\", dropout_ratio=0.25):\n",
        "    #3x3 kernel tower\n",
        "    tower = Conv2D(features_nr, shape, padding='same', activation='relu',\n",
        "                     kernel_regularizer=regularization, \n",
        "                   name='tower_simple_%d_%dx%db'%(tower_nr, shape[0], shape[1]))(input_layer)\n",
        "    #condidional dropout/normalization\n",
        "    if dropout:\n",
        "        tower = Dropout(dropout_ratio, name='tower_%d_%dx%ddrop'%(tower_nr, shape[0], shape[1]))(tower)\n",
        "    if normalization:\n",
        "        tower = BatchNormalization(name='tower_%d_%dx%dnorm'%(tower_nr, shape[0], shape[1]))(tower)\n",
        "        \n",
        "    return tower\n",
        "\n",
        "def build_tower_subsample(input_layer, features_nr, shape, tower_nr, \n",
        "                          dropout=False, normalization=False, regularization='l2', dropout_ratio=0.25):\n",
        "    tower = build_tower(input_layer, features_nr, shape, tower_nr, \n",
        "                        dropout, normalization, regularization, dropout_ratio)\n",
        "    pool = MaxPooling2D((2,2), padding='same', name='tower_%d_2x2subsample'%(tower_nr))(tower)\n",
        "\n",
        "    return pool\n",
        "\n",
        "def build_simple_tower_subsample(input_layer, features_nr, shape, tower_nr, \n",
        "                          dropout=False, normalization=False, regularization='l2', dropout_ratio=0.25):\n",
        "    tower = build_simple_tower(input_layer, features_nr, shape, tower_nr, \n",
        "                        dropout, normalization, regularization, dropout_ratio)\n",
        "    pool = MaxPooling2D((2,2), padding='same', name='tower_%d_2x2subsample'%(tower_nr))(tower)\n",
        "\n",
        "    return pool\n",
        "\n",
        "def build_dense(input_layer, neurons_nr, dense_nr, \n",
        "                dropout=False, normalization=False, regularization='l2', dropout_ratio=0.5):\n",
        "    dense = Dense(neurons_nr, kernel_regularizer=regularization, \n",
        "                  name='dense_%d_%d'%(dense_nr, neurons_nr))(input_layer)\n",
        "    \n",
        "    if dropout:\n",
        "        dense = Dropout(dropout_ratio, name='dense_%d_%ddrop'%(dense_nr, neurons_nr))(dense)\n",
        "    if normalization:\n",
        "        dense = BatchNormalization(name='dense_%d_%dnorm'%(dense_nr, neurons_nr))(dense)\n",
        "    \n",
        "    return dense\n",
        "\n",
        "def build_inception_module(input_layer, features_nr, module_nr, \n",
        "                           dropout=False, normalization=False, \n",
        "                           regularization='l2', dropout_ratio=0.2, use_maxpool=True):  \n",
        "    #feature_nr is an array we'll use to build our layers\n",
        "    #data is in the form: [1x1, 3x3 reduce, 3x3, 5x5 reduce, 5x5, pool proj]\n",
        "  \n",
        "    inception_1x1 = Conv2D(features_nr[0],1,1,border_mode='same',activation='relu',name='inception_%d_/1x1'%(module_nr),W_regularizer=l2(0.0002))(input_layer)\n",
        "    \n",
        "    inception_3x3_reduce = Conv2D(features_nr[1],1,1,border_mode='same',activation='relu',name='inception_%d_/3x3_reduce'%(module_nr),W_regularizer=l2(0.0002))(input_layer)\n",
        "    \n",
        "    inception_3x3 = Conv2D(features_nr[2],3,3,border_mode='same',activation='relu',name='inception_%d_/3x3'%(module_nr),W_regularizer=l2(0.0002))(inception_3x3_reduce)\n",
        "    \n",
        "    inception_5x5_reduce = Conv2D(features_nr[3],1,1,border_mode='same',activation='relu',name='inception_%d_/5x5_reduce'%(module_nr),W_regularizer=l2(0.0002))(input_layer)\n",
        "    \n",
        "    inception_5x5 = Conv2D(features_nr[4],5,5,border_mode='same',activation='relu',name='inception_%d_/5x5'%(module_nr),W_regularizer=l2(0.0002))(inception_5x5_reduce)\n",
        "    \n",
        "    inception_pool = MaxPooling2D(pool_size=(3,3),strides=(1,1),border_mode='same',name='inception_%d_/pool'%(module_nr))(input_layer)\n",
        "    \n",
        "    inception_pool_proj = Conv2D(features_nr[5],1,1,border_mode='same',activation='relu',name='inception_%d_/pool_proj'%(module_nr),W_regularizer=l2(0.0002))(inception_pool)\n",
        "    \n",
        "    inception_output = concatenate([inception_1x1,inception_3x3,inception_5x5,inception_pool_proj],axis=3,name='inception_%d_/output'%(module_nr))\n",
        "\n",
        "    if dropout:\n",
        "        inception_output = Dropout(dropout_ratio, name='inception_%d_/output_drop'%(module_nr))(inception_output)\n",
        "    if normalization:\n",
        "        inception_output = BatchNormalization(name='inception_%d_/output_norm'%(module_nr))(inception_output)\n",
        "\n",
        "    if use_maxpool:\n",
        "      inception_output = MaxPooling2D((2,2), padding='same', name='inception_%d_2x2subsample'%(module_nr))(inception_output)\n",
        "    \n",
        "    return inception_output\n",
        "\n",
        "i='cifar100-nrcrt2-'+datetime.datetime.now().strftime(\"%I:%M%p_%B-%d-%Y\")\n",
        "\n",
        "K.clear_session()\n",
        "\n",
        "!mkdir -p models\n",
        "!mkdir -p logs\n",
        "\n",
        "a = EarlyStopping(monitor='val_loss', min_delta=0, patience=10, verbose=1, mode='auto')#will stop the model if val_loss does not improve for 2 consecutive epochs\n",
        "b = ModelCheckpoint(monitor='val_loss', filepath='./models/'+str(i)+'.hdf5', verbose=1, save_best_only=True)#save model weights after each epoch if val_loss improves\n",
        "c = TensorBoard(log_dir='./logs/'+str(i),\n",
        "                write_grads=True,\n",
        "                write_graph=True,\n",
        "                write_images=True,\n",
        "                batch_size=128)#saves a log file for tensorboard; remember to save different runs to different subdirectories\n",
        "\n",
        "#we'll use this instead of decay\n",
        "d = ReduceLROnPlateau(monitor='val_loss', factor=0.1, patience=5, verbose=0, mode='auto', min_delta=0.0001, cooldown=0, min_lr=0)\n",
        "\n",
        "callbacks=[a,b,c,d]\n",
        "\n",
        "#------------model definition-------------------\n",
        "\n",
        "use_norm = True\n",
        "lrate = 0.001\n",
        "\n",
        "input_img = Input(shape = (32, 32, 3), name='input')\n",
        "\n",
        "#conv_1 = Conv2D(1, (1,1), padding='same', activation='relu', \n",
        "               # kernel_regularizer = regularization, name='conv_64x64x1_inception_in')(input_img)\n",
        "\n",
        "#hopefully this will learn a good internal representation of the image channels\n",
        "#conv_1 = Conv2D(1, (1,1), padding='same', activation='relu', \n",
        "                #kernel_regularizer = regularization, name='conv_64x64x1_inception_in')(input_img)\n",
        "\n",
        "inception_1 = build_inception_module(input_img, [64,96,128,16,32,32], 1, True, use_norm)\n",
        "\n",
        "inception_2 = build_inception_module(inception_1, [128,128,192,32,96,64], 2, True, use_norm)\n",
        "\n",
        "inception_3 = build_inception_module(inception_2, [192,96,208,16,48,64], 3, True, use_norm)\n",
        "\n",
        "#tower_3 = build_simple_tower(inception_2, 144, (3,3),  3, False, use_norm)\n",
        "#tower_4 = build_simple_tower_subsample(tower_3, 144, (3,3), 4, False, use_norm)\n",
        "\n",
        "#tower_5 = build_simple_tower(tower_4, 288, (3,3),  5, False, use_norm)\n",
        "#tower_6 = build_simple_tower_subsample(tower_5, 288, (3,3), 6, False, use_norm)\n",
        "\n",
        "#model top\n",
        "\n",
        "flat_pool = AveragePooling2D(pool_size=(4, 4), padding='valid')(inception_3)\n",
        "\n",
        "flat = Flatten()(flat_pool)\n",
        "\n",
        "\n",
        "dense_5 = build_dense(flat, 128, 1, True, use_norm)\n",
        "\n",
        "dense_6 = build_dense(dense_5, 64, 2, True, use_norm)\n",
        "\n",
        "out = Dense(100, activation='softmax')(dense_6)\n",
        "\n",
        "model = Model(inputs = input_img, outputs = out)\n",
        "\n",
        "#-----------------------------------------------\n",
        "\n",
        "model.compile(loss='categorical_crossentropy',\n",
        "              optimizer=Adam(lrate),\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "model.summary()\n",
        "\n",
        "model_json = model.to_json()\n",
        "with open(\"./models/\"+str(i)+\".json\", \"w\") as json_file:\n",
        "    json_file.write(model_json)\n",
        "\n",
        "print(\"Saved model to\" + \"../models/\"+str(i)+\".json\")\n"
      ],
      "execution_count": 96,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:76: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), activation=\"relu\", name=\"inception_1_/1x1\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:78: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (1, 1), activation=\"relu\", name=\"inception_1_/3x3_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:80: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (3, 3), activation=\"relu\", name=\"inception_1_/3x3\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (1, 1), activation=\"relu\", name=\"inception_1_/5x5_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (5, 5), activation=\"relu\", name=\"inception_1_/5x5\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:86: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), name=\"inception_1_/pool\", padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:88: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), activation=\"relu\", name=\"inception_1_/pool_proj\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:76: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), activation=\"relu\", name=\"inception_2_/1x1\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:78: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(128, (1, 1), activation=\"relu\", name=\"inception_2_/3x3_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:80: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (3, 3), activation=\"relu\", name=\"inception_2_/3x3\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(32, (1, 1), activation=\"relu\", name=\"inception_2_/5x5_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (5, 5), activation=\"relu\", name=\"inception_2_/5x5\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:86: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), name=\"inception_2_/pool\", padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:88: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), activation=\"relu\", name=\"inception_2_/pool_proj\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:76: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(192, (1, 1), activation=\"relu\", name=\"inception_3_/1x1\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:78: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(96, (1, 1), activation=\"relu\", name=\"inception_3_/3x3_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:80: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(208, (3, 3), activation=\"relu\", name=\"inception_3_/3x3\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:82: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(16, (1, 1), activation=\"relu\", name=\"inception_3_/5x5_reduce\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:84: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(48, (5, 5), activation=\"relu\", name=\"inception_3_/5x5\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:86: UserWarning: Update your `MaxPooling2D` call to the Keras 2 API: `MaxPooling2D(pool_size=(3, 3), strides=(1, 1), name=\"inception_3_/pool\", padding=\"same\")`\n",
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:88: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(64, (1, 1), activation=\"relu\", name=\"inception_3_/pool_proj\", padding=\"same\", kernel_regularizer=<keras.reg...)`\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "__________________________________________________________________________________________________\n",
            "Layer (type)                    Output Shape         Param #     Connected to                     \n",
            "==================================================================================================\n",
            "input (InputLayer)              (None, 32, 32, 3)    0                                            \n",
            "__________________________________________________________________________________________________\n",
            "inception_1_/3x3_reduce (Conv2D (None, 32, 32, 96)   384         input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "inception_1_/5x5_reduce (Conv2D (None, 32, 32, 16)   64          input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "inception_1_/pool (MaxPooling2D (None, 32, 32, 3)    0           input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "inception_1_/1x1 (Conv2D)       (None, 32, 32, 64)   256         input[0][0]                      \n",
            "__________________________________________________________________________________________________\n",
            "inception_1_/3x3 (Conv2D)       (None, 32, 32, 128)  110720      inception_1_/3x3_reduce[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "inception_1_/5x5 (Conv2D)       (None, 32, 32, 32)   12832       inception_1_/5x5_reduce[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "inception_1_/pool_proj (Conv2D) (None, 32, 32, 32)   128         inception_1_/pool[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "inception_1_/output (Concatenat (None, 32, 32, 256)  0           inception_1_/1x1[0][0]           \n",
            "                                                                 inception_1_/3x3[0][0]           \n",
            "                                                                 inception_1_/5x5[0][0]           \n",
            "                                                                 inception_1_/pool_proj[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "inception_1_/output_drop (Dropo (None, 32, 32, 256)  0           inception_1_/output[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "inception_1_/output_norm (Batch (None, 32, 32, 256)  1024        inception_1_/output_drop[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "inception_1_2x2subsample (MaxPo (None, 16, 16, 256)  0           inception_1_/output_norm[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "inception_2_/3x3_reduce (Conv2D (None, 16, 16, 128)  32896       inception_1_2x2subsample[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "inception_2_/5x5_reduce (Conv2D (None, 16, 16, 32)   8224        inception_1_2x2subsample[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "inception_2_/pool (MaxPooling2D (None, 16, 16, 256)  0           inception_1_2x2subsample[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "inception_2_/1x1 (Conv2D)       (None, 16, 16, 128)  32896       inception_1_2x2subsample[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "inception_2_/3x3 (Conv2D)       (None, 16, 16, 192)  221376      inception_2_/3x3_reduce[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "inception_2_/5x5 (Conv2D)       (None, 16, 16, 96)   76896       inception_2_/5x5_reduce[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "inception_2_/pool_proj (Conv2D) (None, 16, 16, 64)   16448       inception_2_/pool[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "inception_2_/output (Concatenat (None, 16, 16, 480)  0           inception_2_/1x1[0][0]           \n",
            "                                                                 inception_2_/3x3[0][0]           \n",
            "                                                                 inception_2_/5x5[0][0]           \n",
            "                                                                 inception_2_/pool_proj[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "inception_2_/output_drop (Dropo (None, 16, 16, 480)  0           inception_2_/output[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "inception_2_/output_norm (Batch (None, 16, 16, 480)  1920        inception_2_/output_drop[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "inception_2_2x2subsample (MaxPo (None, 8, 8, 480)    0           inception_2_/output_norm[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "inception_3_/3x3_reduce (Conv2D (None, 8, 8, 96)     46176       inception_2_2x2subsample[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "inception_3_/5x5_reduce (Conv2D (None, 8, 8, 16)     7696        inception_2_2x2subsample[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "inception_3_/pool (MaxPooling2D (None, 8, 8, 480)    0           inception_2_2x2subsample[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "inception_3_/1x1 (Conv2D)       (None, 8, 8, 192)    92352       inception_2_2x2subsample[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "inception_3_/3x3 (Conv2D)       (None, 8, 8, 208)    179920      inception_3_/3x3_reduce[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "inception_3_/5x5 (Conv2D)       (None, 8, 8, 48)     19248       inception_3_/5x5_reduce[0][0]    \n",
            "__________________________________________________________________________________________________\n",
            "inception_3_/pool_proj (Conv2D) (None, 8, 8, 64)     30784       inception_3_/pool[0][0]          \n",
            "__________________________________________________________________________________________________\n",
            "inception_3_/output (Concatenat (None, 8, 8, 512)    0           inception_3_/1x1[0][0]           \n",
            "                                                                 inception_3_/3x3[0][0]           \n",
            "                                                                 inception_3_/5x5[0][0]           \n",
            "                                                                 inception_3_/pool_proj[0][0]     \n",
            "__________________________________________________________________________________________________\n",
            "inception_3_/output_drop (Dropo (None, 8, 8, 512)    0           inception_3_/output[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "inception_3_/output_norm (Batch (None, 8, 8, 512)    2048        inception_3_/output_drop[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "inception_3_2x2subsample (MaxPo (None, 4, 4, 512)    0           inception_3_/output_norm[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "average_pooling2d_1 (AveragePoo (None, 1, 1, 512)    0           inception_3_2x2subsample[0][0]   \n",
            "__________________________________________________________________________________________________\n",
            "flatten_1 (Flatten)             (None, 512)          0           average_pooling2d_1[0][0]        \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_128 (Dense)             (None, 128)          65664       flatten_1[0][0]                  \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_128drop (Dropout)       (None, 128)          0           dense_1_128[0][0]                \n",
            "__________________________________________________________________________________________________\n",
            "dense_1_128norm (BatchNormaliza (None, 128)          512         dense_1_128drop[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_64 (Dense)              (None, 64)           8256        dense_1_128norm[0][0]            \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_64drop (Dropout)        (None, 64)           0           dense_2_64[0][0]                 \n",
            "__________________________________________________________________________________________________\n",
            "dense_2_64norm (BatchNormalizat (None, 64)           256         dense_2_64drop[0][0]             \n",
            "__________________________________________________________________________________________________\n",
            "dense_1 (Dense)                 (None, 100)          6500        dense_2_64norm[0][0]             \n",
            "==================================================================================================\n",
            "Total params: 975,476\n",
            "Trainable params: 972,596\n",
            "Non-trainable params: 2,880\n",
            "__________________________________________________________________________________________________\n",
            "Saved model to../models/cifar100-nrcrt2-11:15AM_July-12-2018.json\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "oZQ5GrZVA1bv",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        },
        "outputId": "14cf4223-c612-4365-d649-c33c5e05d5ce"
      },
      "cell_type": "code",
      "source": [
        "def get_model_memory_usage(batch_size, model):\n",
        "    import numpy as np\n",
        "    from keras import backend as K\n",
        "\n",
        "    shapes_mem_count = 0\n",
        "    for l in model.layers:\n",
        "        single_layer_mem = 1\n",
        "        for s in l.output_shape:\n",
        "            if s is None:\n",
        "                continue\n",
        "            single_layer_mem *= s\n",
        "        shapes_mem_count += single_layer_mem\n",
        "\n",
        "    trainable_count = np.sum([K.count_params(p) for p in set(model.trainable_weights)])\n",
        "    non_trainable_count = np.sum([K.count_params(p) for p in set(model.non_trainable_weights)])\n",
        "\n",
        "    total_memory = 4.0*batch_size*(shapes_mem_count + trainable_count + non_trainable_count)\n",
        "    gbytes = np.round(total_memory / (1024.0 ** 3), 3)\n",
        "    return gbytes\n",
        "  \n",
        "print(\"Memory usage (GB):\", get_model_memory_usage(128,model))"
      ],
      "execution_count": 97,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Memory usage (GB): 1.439\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "JRZf1eVHiKpw",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 2243
        },
        "outputId": "48349f83-bbba-4979-fb46-f56607537052"
      },
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "\n",
        "with tf.device('/gpu:0'):\n",
        "  model.fit_generator(generator=train,\n",
        "                      validation_data=valid,\n",
        "                      steps_per_epoch=2000,\n",
        "                      epochs=50,\n",
        "                      verbose=1,callbacks=callbacks)  # starts training\n",
        "\n",
        "result = model.evaluate_generator(test)\n",
        "\n",
        "print(\"Accuracy on test set: \",result[1]*100,\"%\")\n",
        "\n",
        "#copy our generated model and logs to GoogleDrive\n",
        "!cp -R models my_drive/ai/projects/cifar100\n",
        "!cp -R logs my_drive/ai/projects/cifar100\n",
        "\n",
        "print(\"Copied model and logs to Google Drive\")"
      ],
      "execution_count": 98,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/50\n",
            "1999/2000 [============================>.] - ETA: 0s - loss: 4.2657 - acc: 0.1187"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2000/2000 [==============================] - 301s 150ms/step - loss: 4.2654 - acc: 0.1187 - val_loss: 4.1352 - val_acc: 0.1080\n",
            "\n",
            "Epoch 00001: val_loss improved from inf to 4.13520, saving model to ./models/cifar100-nrcrt2-11:15AM_July-12-2018.hdf5\n",
            "Epoch 2/50\n",
            " 235/2000 [==>...........................] - ETA: 4:17 - loss: 3.5188 - acc: 0.1932"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 3.3263 - acc: 0.2362"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2000/2000 [==============================] - 299s 149ms/step - loss: 3.3262 - acc: 0.2362 - val_loss: 3.5125 - val_acc: 0.2021\n",
            "\n",
            "Epoch 00002: val_loss improved from 4.13520 to 3.51246, saving model to ./models/cifar100-nrcrt2-11:15AM_July-12-2018.hdf5\n",
            "Epoch 3/50\n",
            " 232/2000 [==>...........................] - ETA: 4:17 - loss: 3.1371 - acc: 0.2766"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 3.0621 - acc: 0.2945"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2000/2000 [==============================] - 298s 149ms/step - loss: 3.0620 - acc: 0.2946 - val_loss: 3.5015 - val_acc: 0.2282\n",
            "\n",
            "Epoch 00003: val_loss improved from 3.51246 to 3.50152, saving model to ./models/cifar100-nrcrt2-11:15AM_July-12-2018.hdf5\n",
            "Epoch 4/50\n",
            " 232/2000 [==>...........................] - ETA: 4:17 - loss: 3.0009 - acc: 0.3151"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 2.9191 - acc: 0.3319"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2000/2000 [==============================] - 299s 149ms/step - loss: 2.9190 - acc: 0.3319 - val_loss: 3.2686 - val_acc: 0.2799\n",
            "\n",
            "Epoch 00004: val_loss improved from 3.50152 to 3.26859, saving model to ./models/cifar100-nrcrt2-11:15AM_July-12-2018.hdf5\n",
            "Epoch 5/50\n",
            " 232/2000 [==>...........................] - ETA: 4:18 - loss: 2.8417 - acc: 0.3519"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 2.8185 - acc: 0.3574"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 299s 149ms/step - loss: 2.8185 - acc: 0.3574 - val_loss: 3.5001 - val_acc: 0.2369\n",
            "\n",
            "Epoch 00005: val_loss did not improve from 3.26859\n",
            "Epoch 6/50\n",
            " 365/2000 [====>.........................] - ETA: 3:58 - loss: 2.7628 - acc: 0.3729"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 2.7469 - acc: 0.3771"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2000/2000 [==============================] - 298s 149ms/step - loss: 2.7469 - acc: 0.3770 - val_loss: 3.1567 - val_acc: 0.3125\n",
            "\n",
            "Epoch 00006: val_loss improved from 3.26859 to 3.15667, saving model to ./models/cifar100-nrcrt2-11:15AM_July-12-2018.hdf5\n",
            "Epoch 7/50\n",
            " 232/2000 [==>...........................] - ETA: 4:17 - loss: 2.6945 - acc: 0.3869"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 2.6886 - acc: 0.3914"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 299s 149ms/step - loss: 2.6886 - acc: 0.3914 - val_loss: 3.2248 - val_acc: 0.2894\n",
            "\n",
            "Epoch 00007: val_loss did not improve from 3.15667\n",
            "Epoch 8/50\n",
            " 365/2000 [====>.........................] - ETA: 3:58 - loss: 2.6821 - acc: 0.3994"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 2.6444 - acc: 0.4035"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2000/2000 [==============================] - 299s 149ms/step - loss: 2.6444 - acc: 0.4035 - val_loss: 3.0028 - val_acc: 0.3429\n",
            "\n",
            "Epoch 00008: val_loss improved from 3.15667 to 3.00279, saving model to ./models/cifar100-nrcrt2-11:15AM_July-12-2018.hdf5\n",
            "Epoch 9/50\n",
            " 232/2000 [==>...........................] - ETA: 4:18 - loss: 2.6524 - acc: 0.4075"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 2.6114 - acc: 0.4137"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 298s 149ms/step - loss: 2.6113 - acc: 0.4138 - val_loss: 3.2116 - val_acc: 0.3036\n",
            "\n",
            "Epoch 00009: val_loss did not improve from 3.00279\n",
            "Epoch 10/50\n",
            " 364/2000 [====>.........................] - ETA: 3:57 - loss: 2.5875 - acc: 0.4210"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 2.5809 - acc: 0.4214"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2000/2000 [==============================] - 298s 149ms/step - loss: 2.5808 - acc: 0.4214 - val_loss: 2.9458 - val_acc: 0.3558\n",
            "\n",
            "Epoch 00010: val_loss improved from 3.00279 to 2.94576, saving model to ./models/cifar100-nrcrt2-11:15AM_July-12-2018.hdf5\n",
            "Epoch 11/50\n",
            " 231/2000 [==>...........................] - ETA: 4:17 - loss: 2.5235 - acc: 0.4342"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 2.5465 - acc: 0.4331"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 298s 149ms/step - loss: 2.5464 - acc: 0.4331 - val_loss: 2.9643 - val_acc: 0.3527\n",
            "\n",
            "Epoch 00011: val_loss did not improve from 2.94576\n",
            "Epoch 12/50\n",
            " 364/2000 [====>.........................] - ETA: 3:58 - loss: 2.5469 - acc: 0.4374"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 2.5248 - acc: 0.4395"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 297s 148ms/step - loss: 2.5247 - acc: 0.4395 - val_loss: 3.1614 - val_acc: 0.3128\n",
            "\n",
            "Epoch 00012: val_loss did not improve from 2.94576\n",
            "Epoch 13/50\n",
            " 364/2000 [====>.........................] - ETA: 3:58 - loss: 2.5497 - acc: 0.4355"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 2.5059 - acc: 0.4452"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 298s 149ms/step - loss: 2.5059 - acc: 0.4452 - val_loss: 3.1679 - val_acc: 0.3167\n",
            "\n",
            "Epoch 00013: val_loss did not improve from 2.94576\n",
            "Epoch 14/50\n",
            " 364/2000 [====>.........................] - ETA: 3:58 - loss: 2.4963 - acc: 0.4471"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 2.4876 - acc: 0.4506"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 298s 149ms/step - loss: 2.4877 - acc: 0.4506 - val_loss: 3.0471 - val_acc: 0.3388\n",
            "\n",
            "Epoch 00014: val_loss did not improve from 2.94576\n",
            "Epoch 15/50\n",
            " 364/2000 [====>.........................] - ETA: 3:58 - loss: 2.4609 - acc: 0.4605"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 2.4703 - acc: 0.4575"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2000/2000 [==============================] - 298s 149ms/step - loss: 2.4704 - acc: 0.4574 - val_loss: 3.2161 - val_acc: 0.3154\n",
            "\n",
            "Epoch 00015: val_loss did not improve from 2.94576\n",
            "Epoch 16/50\n",
            " 279/2000 [===>..........................] - ETA: 4:09 - loss: 2.2858 - acc: 0.4983"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 2.1324 - acc: 0.5306"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2000/2000 [==============================] - 298s 149ms/step - loss: 2.1324 - acc: 0.5306 - val_loss: 2.8119 - val_acc: 0.3869\n",
            "\n",
            "Epoch 00016: val_loss improved from 2.94576 to 2.81186, saving model to ./models/cifar100-nrcrt2-11:15AM_July-12-2018.hdf5\n",
            "Epoch 17/50\n",
            " 232/2000 [==>...........................] - ETA: 4:18 - loss: 1.9893 - acc: 0.5560"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 1.9459 - acc: 0.5643"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 298s 149ms/step - loss: 1.9458 - acc: 0.5643 - val_loss: 2.8124 - val_acc: 0.3787\n",
            "\n",
            "Epoch 00017: val_loss did not improve from 2.81186\n",
            "Epoch 18/50\n",
            " 364/2000 [====>.........................] - ETA: 3:58 - loss: 1.9074 - acc: 0.5650"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 1.8646 - acc: 0.5739"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2000/2000 [==============================] - 298s 149ms/step - loss: 1.8646 - acc: 0.5739 - val_loss: 2.7401 - val_acc: 0.3889\n",
            "\n",
            "Epoch 00018: val_loss improved from 2.81186 to 2.74011, saving model to ./models/cifar100-nrcrt2-11:15AM_July-12-2018.hdf5\n",
            "Epoch 19/50\n",
            " 232/2000 [==>...........................] - ETA: 4:18 - loss: 1.8133 - acc: 0.5849"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 1.7853 - acc: 0.5887"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 298s 149ms/step - loss: 1.7852 - acc: 0.5887 - val_loss: 2.7499 - val_acc: 0.3830\n",
            "\n",
            "Epoch 00019: val_loss did not improve from 2.74011\n",
            "Epoch 20/50\n",
            " 364/2000 [====>.........................] - ETA: 3:58 - loss: 1.7555 - acc: 0.5924"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 1.7383 - acc: 0.5941"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2000/2000 [==============================] - 298s 149ms/step - loss: 1.7380 - acc: 0.5942 - val_loss: 2.7171 - val_acc: 0.3848\n",
            "\n",
            "Epoch 00020: val_loss improved from 2.74011 to 2.71710, saving model to ./models/cifar100-nrcrt2-11:15AM_July-12-2018.hdf5\n",
            "Epoch 21/50\n",
            " 232/2000 [==>...........................] - ETA: 4:17 - loss: 1.7148 - acc: 0.5962"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 1.6950 - acc: 0.6018"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2000/2000 [==============================] - 298s 149ms/step - loss: 1.6949 - acc: 0.6019 - val_loss: 2.6815 - val_acc: 0.3924\n",
            "\n",
            "Epoch 00021: val_loss improved from 2.71710 to 2.68147, saving model to ./models/cifar100-nrcrt2-11:15AM_July-12-2018.hdf5\n",
            "Epoch 22/50\n",
            " 232/2000 [==>...........................] - ETA: 4:18 - loss: 1.6767 - acc: 0.6070"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 1.6617 - acc: 0.6072"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 298s 149ms/step - loss: 1.6617 - acc: 0.6072 - val_loss: 2.7635 - val_acc: 0.3805\n",
            "\n",
            "Epoch 00022: val_loss did not improve from 2.68147\n",
            "Epoch 23/50\n",
            " 364/2000 [====>.........................] - ETA: 3:58 - loss: 1.6530 - acc: 0.6045"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 1.6279 - acc: 0.6125"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 298s 149ms/step - loss: 1.6280 - acc: 0.6125 - val_loss: 2.7361 - val_acc: 0.3831\n",
            "\n",
            "Epoch 00023: val_loss did not improve from 2.68147\n",
            "Epoch 24/50\n",
            " 364/2000 [====>.........................] - ETA: 3:58 - loss: 1.6169 - acc: 0.6189"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 1.6028 - acc: 0.6191"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 298s 149ms/step - loss: 1.6028 - acc: 0.6191 - val_loss: 2.7213 - val_acc: 0.3849\n",
            "\n",
            "Epoch 00024: val_loss did not improve from 2.68147\n",
            "Epoch 25/50\n",
            " 364/2000 [====>.........................] - ETA: 3:57 - loss: 1.5804 - acc: 0.6215"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 1.5819 - acc: 0.6221"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 298s 149ms/step - loss: 1.5819 - acc: 0.6221 - val_loss: 2.6984 - val_acc: 0.3903\n",
            "\n",
            "Epoch 00025: val_loss did not improve from 2.68147\n",
            "Epoch 26/50\n",
            " 364/2000 [====>.........................] - ETA: 3:58 - loss: 1.5643 - acc: 0.6277"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 1.5573 - acc: 0.6276"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 299s 149ms/step - loss: 1.5574 - acc: 0.6276 - val_loss: 2.7332 - val_acc: 0.3809\n",
            "\n",
            "Epoch 00026: val_loss did not improve from 2.68147\n",
            "Epoch 27/50\n",
            " 364/2000 [====>.........................] - ETA: 3:59 - loss: 1.5306 - acc: 0.6325"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 1.5112 - acc: 0.6386"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 298s 149ms/step - loss: 1.5113 - acc: 0.6386 - val_loss: 2.6924 - val_acc: 0.3894\n",
            "\n",
            "Epoch 00027: val_loss did not improve from 2.68147\n",
            "Epoch 28/50\n",
            " 364/2000 [====>.........................] - ETA: 3:58 - loss: 1.4985 - acc: 0.6428"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 1.4895 - acc: 0.6450"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 299s 149ms/step - loss: 1.4896 - acc: 0.6450 - val_loss: 2.6998 - val_acc: 0.3890\n",
            "\n",
            "Epoch 00028: val_loss did not improve from 2.68147\n",
            "Epoch 29/50\n",
            " 364/2000 [====>.........................] - ETA: 3:58 - loss: 1.4855 - acc: 0.6458"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 1.4795 - acc: 0.6468"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 298s 149ms/step - loss: 1.4795 - acc: 0.6468 - val_loss: 2.7008 - val_acc: 0.3906\n",
            "\n",
            "Epoch 00029: val_loss did not improve from 2.68147\n",
            "Epoch 30/50\n",
            " 364/2000 [====>.........................] - ETA: 3:58 - loss: 1.4677 - acc: 0.6517"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 1.4651 - acc: 0.6519"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "2000/2000 [==============================] - 298s 149ms/step - loss: 1.4651 - acc: 0.6519 - val_loss: 2.6961 - val_acc: 0.3912\n",
            "\n",
            "Epoch 00030: val_loss did not improve from 2.68147\n",
            "Epoch 31/50\n",
            " 364/2000 [====>.........................] - ETA: 3:59 - loss: 1.4702 - acc: 0.6465"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "1999/2000 [============================>.] - ETA: 0s - loss: 1.4636 - acc: 0.6493"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\b\r2000/2000 [==============================] - 298s 149ms/step - loss: 1.4636 - acc: 0.6493 - val_loss: 2.7162 - val_acc: 0.3860\n",
            "\n",
            "Epoch 00031: val_loss did not improve from 2.68147\n",
            "Epoch 00031: early stopping\n",
            "Accuracy on test set:  39.589999999999996 %\n",
            "Copied model and logs to Google Drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "f53m3bg_p5rk",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        "# Model evaluation"
      ]
    },
    {
      "metadata": {
        "id": "TqgxBsf7j3lD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        },
        "outputId": "bb2a5db0-1b70-468a-e0ee-b63bab2de8d7"
      },
      "cell_type": "code",
      "source": [
        "model.load_weights('./models/cifar10-nrcrt10-03:02PM_July-11-2018.hdf5')\n",
        "\n",
        "result = model.evaluate_generator(test)\n",
        "\n",
        "print(result)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "10000/10000 [==============================] - 8s 837us/step\n",
            "[0.12198813806772232, 0.9664000005722045]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}